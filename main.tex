\documentclass[12pt]{article}

\usepackage{fullpage,amssymb,amsthm,amsmath}

\newtheorem{lemma}{Lemma}

\newcommand{\R}{\mathbb{R}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\indicator}{\mathbf{1}}
\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\Exp}{\mathbf{E}}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}

\begin{document}

\title{Quantile Bandits}
\author{Francesco Orabona \and D\'avid P\'al}

\maketitle

\section{Introduction}

The non-stochastic multi-armed problem is an online problem where an algorithm
in round $t$ chooses an \emph{action}/\emph{arm} $I_t \in \{1,2,\dots,K\}$ and
gets to observe its reward $r_{t,I_t} \in [0,1]$. The goal of the algorithm is
to maximize its reward $\sum_{t=1}^T r_{t,I_t}$ accumulated over $T$ rounds.
Each arm $i$ in each round $t$ has an associated reward $r_{t,i} \in [0,1]$.
For simplicity, we assume that all rewards for all arms were chosen before the
first round. This assumption is called the model with \emph{oblivious
adversary}.

Let $\Delta_K = \{ x \in \R^K ~:~ x \ge 0, \norm{x}_1 = 1 \}$ be the
$K$-dimensional probability simplex. For any probability vector $u \in
\Delta_K$, we consider a (hypothetical) strategy that in each round $t$ chooses
arm randomly from $u$. It's expected reward after $T$ rounds is $\sum_{t=1}^T
\langle r_t, u \rangle$.

We compare an algorithm with a strategy associated with $u \in \Delta_K$.
The difference of algorithm's reward and strategy's reward is called \emph{regret
with respect to $u$}, and it is defined as
$$
\Regret_T(u) = \sum_{t=1}^T \langle r_t, u \rangle - \sum_{t=1}^T r_{t,I_t} \; .
$$
The regret with respect to best $u$ is denoted as
$$
\Regret_T =  \sup_{u \in \Delta_K} \Regret_T(u) \; .
$$
Most algorithms for multi-armed bandit are randomized, hence algorithm's
choices of arms $I_1, I_2, \dots, I_T$ as well as $\Regret_T(u)$ and
$\Regret_T$ are random variables. To evaluate performance of an algorithm,
we might thus use $\Exp[\Regret_T(u)]$ and $\Exp[\Regret_T]$.

For any $K$ and any $T$, there exist an algorithm such that $\Exp[\Regret_T] \le
100 \sqrt{KT}$. This a great result, but we like to improve it. Specifically,
for any $K$ and any $T$, we wish to design an algorithm such that
\begin{equation}
\label{equation:generic-adaptive-bound}
\forall u \in \Delta_N \qquad \qquad \Exp[\Regret_T(u)] \le F(u) \sqrt{T} \; ,
\end{equation}
where $F:\Delta_K \to \R_+$ is a suitable function. Such result would be
interesting if $\sup_{u \in \Delta_K} F(u) \le O(\sqrt{K})$ and for certain
$u$, $F(u)$ would be much smaller than $O(\sqrt{K})$. We refer to a bound of the
form~\eqref{equation:generic-adaptive-bound} as an adaptive bound.

\section{Quantile Bound}

To get an idea what type of adaptive bound might be possible. Consider the
following idea: Instead of having an algorithm compete with the best of the $K$
arms, it might compete only with best $(\epsilon K)$-th best arm, for some
$\epsilon \in (0,1)$. An almost equivalent way of stating this, the algorithm
can be competing with all competitors $u \in \Delta_K$ such that
$\norm{u}_\infty \le \frac{1}{\epsilon K}$.

A simple algorithm that achieves the first goal to simply draw $N = \lceil
\frac{\log T}{\epsilon} \rceil$ arms $j_1, j_2, \dots, j_N \in \{1,2, \dots,
K\}$ independently uniformly at random before the first round, and use a
Bubeck's algorithm on this set of arms. It is not hard to see that the expected
regret of this algorithm w.r.t. $(\epsilon K)$-th best arm is $O(\sqrt{N T}) =
O \left(\sqrt{\frac{1}{\epsilon} T \log T} \right)$.

To see this, note that with probability $1 - (1-\epsilon)^N \ge 1 -
e^{-\epsilon N} \ge 1 - 1/T$, one of top $\epsilon K$ best arms is in set
$\{j_1, j_2, \dots, j_N\}$. Conditioned on this event, the expected regret
w.r.t.  any arm in $\{j_1, j_2, \dots, j_N\}$ is $O(\sqrt{NT}) =
O\left(\sqrt{\frac{1}{\epsilon} T \log T}\right)$.  If none of the top $\epsilon K$
arms is in the set $\{j_1, j_2, \dots, j_N\}$, we upper bound regret by $T$.
Since this happens with probability at most $1/T$, the contribution towards
expected regret is $1$.

Ignoring this extra $\log T$ factor, we can interpret $\sqrt{1/\epsilon}$ as
$F(u)$ for $u \in \Delta_K$ which is uniform over the top $\epsilon K$ arms.
Of course, the problem with this algorithm is that it needs to know $\epsilon$.
In other words, the set of $u$'s with which the algorithm is competing is only
a subset of $\Delta_K$ that depends on $\epsilon$.

\section{FTRL Based Algorithms}

We consider algorithms based on \emph{Follow The Regularized Leader} algorithm.
The algorithm maintains an estimate of the sum of rewards $\widetilde R_t =
(\widetilde R_{t,1}, \widetilde R_{t,2}, \dots, \widetilde R_{t,K})$ where $\widetilde R_{t,i}$
is an estimate of $\sum_{s=1}^t r_{t,i}$.
In round $t$, the algorithm chooses a distribution $p_t \in \Delta_K$ which
is defined as
$$
p_t = \argmin_{p \in \Delta_K} \left( \Phi(u) - \langle \widetilde R_{t-1}, p \rangle \right) \; .
$$
Then, it chooses an arm $I_t$ according to $p_t$ and observes the reward $r_{t,I_I}$.
Based on $p_t, I_t, r_{t,I_t}$, the algorithm constructs an estimate of the reward vector $r_t = (r_{t,1}, r_{t,2}, \dots, r_{t,K})$.
The estimate is $\widetilde r_t = (\widetilde r_{t,1}, \widetilde r_{t,2}, \dots, \widetilde r_{t,K})$
where the $i$-th coordinate is
$$
\widetilde r_{t,i} = \frac{r_{t,i} \indicator[i=I_t]}{p_{t,i}}  \; .
$$
Finally, the algorithm updates $\widetilde R_t = \widetilde R_{t-1} + \widetilde r_t$.

The function $\Phi:\Delta_K \to \R$ is called a \emph{regularizer}. We assume
that it is convex.  The following key lemma is used to bound algorithm's
regret. The lemma is stated in terms of Fenchel conjugate of $\Phi$ which is
defined as $\Phi^*(\theta) = \sup_{u \in \Delta_N} \langle \theta, u \rangle -
\Phi(u)$.

\begin{lemma}
\label{lemma:ftrl-regret}
Algorithm based on a regularizer $\Phi:\Delta_K \to \R$ satisfies
for any $u \in \Delta_N$ and any $T \ge 0$,
\begin{equation}
\label{equation:ftrl-regret}
\sum_{t=1}^T \langle \widetilde r_t, u \rangle - \sum_{t=1}^T \langle \widetilde r_t, p_t \rangle
\le \Phi(u) + \Phi^*(0) + \sum_{t=1}^T \Phi^*\left(\widetilde R_t\right) - \Phi^*\left(\widetilde R_{t-1}\right) - \langle \widetilde r_t, p_t \rangle \; .
\end{equation}
\end{lemma}

\begin{proof}
We have
\begin{align*}
\sum_{t=1}^T \Phi^*\left(\widetilde R_t\right) - \Phi^*\left(\widetilde R_{t-1}\right)
& = \Phi^*\left(\widetilde R_t \right) - \Phi^*(0) \\
& \ge \langle \widetilde R_T, u \rangle -  \Phi(u) - \Phi^*(0)
\end{align*}
Subtracting $\sum_{t=1}^T \langle \widetilde r_t, p_t \rangle$ from both sides
and rearranging the terms finishes the proof.
\end{proof}

\begin{lemma}
\label{lemma:ftrl-expected-regret}
Algorithm based on a regularizer $\Phi:\Delta_K \to \R$ satisfies
for any $u \in \Delta_N$ and any $T \ge 0$,
\begin{align*}
\Exp[\Regret_T(u)]
& \le \Phi(u) + \Phi^*(0) + \sum_{t=1}^T \Exp\left[\Phi^* \left(\widetilde R_t\right) - \Phi^*\left(\widetilde R_{t-1}\right) - \langle \widetilde r_t, p_t \rangle \right] \\
& \le \Phi(u) + \Phi^*(0) + \sum_{t=1}^T \Exp\left[\Phi^* \left(\widetilde R_t\right) - \Phi^*\left(\widetilde R_{t-1}\right) - \langle r_t, p_t \rangle \right] \; .
\end{align*}
\end{lemma}

\begin{proof}
We apply Lemma~\ref{lemma:ftrl-regret} and take expectation of both sides of \eqref{equation:ftrl-regret}.
The lemma will follow. We just need to verify three equalities
\begin{align}
\Exp[\langle \widetilde r_t, u \rangle] & = \langle r_t, u \rangle \label{equation:expectation-1} \\
\Exp[\langle \widetilde r_t, p_t \rangle] & = \Exp[r_{t,I_t}] = \Exp[\langle r_t, p_t \rangle] \label{equation:expectation-2} \; .
\end{align}

Equality \eqref{equation:expectation-1} holds since
\begin{align*}
\Exp[\langle \widetilde r_t, u \rangle ~|~ I_1, I_2, \dots, I_{t-1}]
& = \sum_{i=1}^T \Exp[\widetilde r_{t,i} ~|~ I_1, I_2, \dots, I_{t-1} ] \cdot u_i \\
& = \sum_{i=1}^T \Exp\left[  \frac{r_{t,i} \indicator[I_t=i]}{p_{t,i}} ~\middle|~ I_1, I_2, \dots, I_{t-1} \right] \cdot u_i \\
& = \sum_{i=1}^T \frac{r_{t,i}}{p_{t,i}} \cdot \Exp\left[ \indicator[I_t=i] ~\middle|~ I_1, I_2, \dots, I_{t-1} \right] \cdot u_i \\
& = \sum_{i=1}^T \frac{r_{t,i}}{p_{t,i}} \cdot p_{t,i}  \cdot u_i \\
& = \langle r_t, u \rangle \\
\end{align*}
where we used that (the random value of) $p_t$ is determined by $I_1, I_2, \dots, I_{t-1}$.

The first equality in \eqref{equation:expectation-2} holds since
$$
\langle \widetilde r_t, p_t \rangle
= \sum_{i=1}^K \widetilde r_{t,i} \cdot p_{t,i}
= \sum_{i=1}^K \frac{r_{t,i} \indicator[i=I_t]}{p_{t,i}} \cdot p_{t,i}
= \sum_{i=1}^K r_{t,i} \indicator[i=I_t]
= r_{t,I_t} \; .
$$
The second equality in \eqref{equation:expectation-2} holds since
\begin{align*}
\Exp\left[ \langle r_t, p_t \rangle ~|~ I_1, I_2, \dots, I_{t-1} \right]
& = \langle r_t, p_t \rangle \\
& = \sum_{i=1}^K r_{t,i} \cdot \Pr\left[ I_t = i ~|~ I_1, I_2, \dots, I_{t-1} \right] \\
& = \Exp\left[r_{t,I_t} ~|~ I_1, I_2, \dots, I_t \right]
\end{align*}
where we used that (the random value of) $p_t$ is determined by $I_1, I_2, \dots, I_{t-1}$.
\end{proof}

\section{Separable Regularizers}

We consider regularizers $\Phi:\Delta_K \to \R$ that are \emph{separable}.
That means $\Phi$ is of the form
$$
\Phi(u) = \sum_{i=1}^K \phi(u_i)
$$
where $\phi:[0,\infty) \to \R$. We will assume that $\phi$
is a convex twice continuously differentiable
function such that
\begin{equation}
\label{equation:limit}
\lim_{z \to 0^+} \phi'(z) = -\infty \; .
\end{equation}
The Fenchel conjugate of the function is
$$
\Phi^*(\theta) = \sup_{u \in \Delta_K} \langle \theta, u \rangle - \Phi(u) \; .
$$
To compute the Fenchel conjugate, we need to solve the optimization problem
$$
\begin{aligned}
& \underset{u \in \R^K}{\text{maximize}}
& & \langle \theta, u \rangle - \Phi(u) \\
& \text{subject to} & & \sum_{i=1}^K u_i = 1 \\
& & & u_i \ge 0 \\
\end{aligned}
$$
Substituting $\Phi(u) = \sum_{i=1}^K \phi(u_i)$, we get
$$
\begin{aligned}
& \underset{u \in \R^K}{\text{maximize}}
& & \sum_{i=1}^K \theta_i u_i - \phi(u_i) \\
& \text{subject to} & & \sum_{i=1}^K u_i = 1 \\
& & & u_i \ge 0 \\
\end{aligned}
$$
We can solve the problem using Karush-Kuhn-Tucker conditions, which are
\begin{align*}
& \theta_i - \phi'(u_i) = \lambda - \alpha_i & \text{(stationarity)} \\
& \sum_{i=1}^N u_i = 1 & \text{(primal feasibility)} \\
& u_i \ge 0 & \text{(primal feasibility)} \\
& \alpha_i \ge 0 & \text{(dual feasibility)} \\
& \alpha_i u_i = 0 & \text{(complementary slackness)}
\end{align*}
Stationarity together with \eqref{equation:limit} implies
that $u_i > 0$. Complementary slackness implies that $\alpha_i = 0$
and thus
$$
u_i = (\phi')^{-1}\left(\theta_i - \lambda \right) \; .
$$

\section{Negative Tsallis Entropy}

Tsallis entropy of order $\alpha$
of a probability distribution $p \in \Delta_K$ is
$$
S_\alpha(p) = \frac{1}{\alpha - 1} \left(1 - \sum_{i=1}^K p_i^\alpha \right) \; .
$$
For $\alpha = 1$, $S_\alpha(p)$ can extended by continuity. It turns
out that
$$
\lim_{\alpha \to 1} S_\alpha(p) = - \sum_{i=1}^N p_i \ln p_i \; .
$$
In other words, $S_1(p)$ is the Shannon entropy of $p$.

We will use a negative multiple of Tsallis entropy as the regularizer
\begin{equation}
\label{equation:negative-tsallis-entropy-regularizer}
\Phi(u) = - \eta S_\alpha(u) \; .
\end{equation}
The parameter $\eta > 0$ is the learning rate. We will choose $\alpha \in (0,1)$
and $\eta$ later. The regularizer \eqref{equation:negative-tsallis-entropy-regularizer}
is separable since it can be written as
$$
\Phi(u) = - \eta S_\alpha(u) = \sum_{i=1}^N \phi(u_i)
\qquad \text{where} \qquad
\phi(z) = \frac{\eta}{1 - \alpha} \left( z - z^\alpha \right) \; .
$$
The derivate $\phi(z)$ is $\phi'(z) = \frac{\eta}{1-\alpha}\left(1 - \alpha z^{\alpha - 1} \right)$
and thus \eqref{equation:limit} is satisfied for $\alpha \in (0,1)$.

\subsection{Regret Bound}

Abernethy-Lee-Tewari 2016 show that if $r_t \in [-1,0]^K$, then FTRL with regularizer
\eqref{equation:negative-tsallis-entropy-regularizer} satisfies
$$
\Exp\left[\Phi^* \left(\widetilde R_t\right) - \Phi^*\left(\widetilde R_{t-1}\right) - \langle \widetilde r_t, p_t \rangle \right] \le \frac{K^\alpha}{\eta \alpha} \; .
$$
Therefore,
$$
\Exp[\Regret_T(u)] \le \Phi(u) + \Phi^*(0) + \frac{K^\alpha T}{\eta \alpha} \; .
$$
Now,
\begin{align*}
\Phi^*(0)
& = - \min_{u \in \Delta_K} \Phi(u) \\
& = \max_{u \in \Delta_K} \eta S_\alpha(u) \\
& = \eta S_\alpha(1/K, 1/K, \dots, 1/K) \\
& = \eta \frac{1}{\alpha - 1} \left(1 - K^{1-\alpha} \right) \; ,
\end{align*}
where we used that by symmetry the maximum of $S_\alpha(\cdot)$ is attained at
$u = (1/K, 1/K, \dots, 1/K)$.
\begin{align*}
\Exp[\Regret_T(u)]
& \le \Phi(u) + \Phi^*(0) + \frac{N^\alpha T}{\eta \alpha} \\
& = - \eta S_\alpha(u) + \eta S_\alpha(1/K, 1/K, \dots, 1/K) + \frac{K^\alpha T}{\eta \alpha} \\
& = \frac{\eta}{1 - \alpha} \left(K^{1-\alpha} - \sum_{i=1}^K u_i^\alpha \right) + \frac{K^\alpha T}{\eta \alpha} \; .
\end{align*}

Suppose that we want know $u \in \Delta_K$ with which we want to compete.
The optimal learning rate is
$$
\eta = \sqrt{\frac{K^\alpha T (1-\alpha)}{\alpha (K^{1-\alpha} - \sum_{i=1}^K u_i^\alpha)}} \; .
$$
and the regret bound becomes
$$
\Exp[\Regret_T(u)]
\le 2\sqrt{\frac{T \cdot K^\alpha \cdot \left( K^{1-\alpha} - \sum_{i=1}^K u_i^\alpha \right) }{\alpha (1-\alpha)}}
$$

\end{document}
